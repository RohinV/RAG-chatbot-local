# ğŸ¤– RAG Chatbot â€” Local Document Q&A with FAISS + Ollama

A fully local **Retrieval-Augmented Generation (RAG)** chatbot that lets you upload documents (PDF, TXT, MD), embed them using **Ollama embeddings**, index them with **FAISS**, and ask questions using local open-source LLMs like:

- DeepSeek R1
- LLaMA 3
- Qwen 3
- Mistral 7B

Everything runs locally â€” no external APIs.

---

## ğŸš€ Features

### ğŸ” Document-Aware Q&A
Ask questions based on your own uploaded documents.

### ğŸ“š Local Vector Search
High-performance similarity search powered by **FAISS**.

### ğŸ§  Local LLM Inference
Uses Ollama to run all AI models locally.

### ğŸ§© Model Switching
Choose from multiple models:
- `deepseek-r1:8b`
- `llama3:latest`
- `qwen3:8b`
- `mistral:7b`

### ğŸ§· Intelligent RAG Prompting
- Avoids hallucination  
- Adds LLM general knowledge when needed  
- Clearly states when info is *not* from documents  
- Clear but not overly verbose answers  

### ğŸ—‚ï¸ Document Upload & Memory
Upload PDFs/TXT/MD and chat with them persistently.

### ğŸ–¥ï¸ Streamlit UI
Clean and user-friendly interface.

---

## ğŸ“¦ Project Structure

```
rag_chatbot/
â”‚â”€â”€ app.py                # Streamlit UI
â”‚â”€â”€ rag.py                # RAG pipeline: LLM + retriever + prompt
â”‚â”€â”€ ingest.py             # Document ingestion & FAISS index creation
â”‚â”€â”€ config.py             # Paths and model configuration
â”‚â”€â”€ requirements.txt      # Python dependencies
â”‚â”€â”€ environment.yml       # Conda environment file
â”‚â”€â”€ faiss_store/          # Local FAISS index
â”‚â”€â”€ docs/                 # Uploaded documents
â”‚â”€â”€ README.md
â”‚â”€â”€ .gitignore
```

---

## ğŸ› ï¸ Installation

### 1ï¸âƒ£ Create Conda Environment

```bash
conda env create -f environment.yml
conda activate rag_chatbot
```

Or install via pip:

```bash
pip install -r requirements.txt
```

---

## 2ï¸âƒ£ Install Ollama

Download Ollama:

https://ollama.com/download

Then pull the models you want:

```bash
ollama pull deepseek-r1:8b
ollama pull llama3
ollama pull qwen3:8b
ollama pull mistral:7b
```

---

## ğŸ“¥ Ingest Documents

Place your documents in the `docs/` folder or upload them in the Streamlit sidebar.

Then run:

```bash
python ingest.py
```

This will:
- Load documents  
- Split into text chunks  
- Create embeddings  
- Build a FAISS index  

---

## ğŸ’¬ Run the Chatbot

```bash
streamlit run app.py
```

Open:

```
http://localhost:8501
```

---

## ğŸ§  How It Works (RAG Pipeline)

1. Load documents (PDF/TXT/MD)  
2. Split text using recursive chunking  
3. Generate embeddings using Ollama  
4. Store embeddings in FAISS  
5. Retrieve relevant chunks  
6. Pass chunks + question into your selected LLM  
7. LLM answers while distinguishing:
   - Document-based info  
   - General knowledge  

---

## ğŸ§ª Example Q&A

**Q:**  
*What is Adam optimization?*

**A:**  
Adam is a stochastic gradient optimization method that combines ideas from AdaGrad and RMSProp.  
*(This information was found in your documents.)*

Additionally, based on general LLM knowledge, Adam is widely used in neural network training due to adaptive learning rates.  
*(This part is not from your documents.)*

---

## ğŸ› ï¸ Troubleshooting

### âŒ FAISS index not found
Run:
```bash
python ingest.py
```

### âŒ Model not found
Pull the model:
```bash
ollama pull deepseek-r1:8b
```

### âŒ Streamlit not updating
Restart:
```bash
streamlit run app.py
```

---

## ğŸ§¾ License
This project is open-source and free to use or modify.

---

## â­ Optional Enhancements
- Chat history saving  
- Dockerfile  
- Hybrid search (FAISS + BM25)  
- GPU acceleration  
- UI redesign  
- Conversation memory

